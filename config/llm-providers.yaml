# Multi-Provider LLM Configuration
# This file configures multiple LLM providers including local (Ollama) and free API providers

llm:
  # Default provider selection strategy
  defaultProvider: "ollama"
  
  # Fallback chain - providers to try in order if primary fails
  fallbackChain: ["ollama", "groq", "huggingface", "together"]
  
  # Request timeout and retry configuration
  timeout: 30000  # 30 seconds
  maxRetries: 3
  retryDelay: 1000  # 1 second
  
  # Rate limiting configuration
  rateLimit:
    enabled: true
    requestsPerMinute: 60
    tokensPerMinute: 100000

  # LLM configuration for input classification
  classifier:
    provider: "ollama"
    model: "qwen3:8b"  # Available model for classification
    temperature: 0.1  # Low temperature for consistent classification
    maxTokens: 200    # Short responses for classification

  # LLM configuration for user prompts (user-changeable)
  prompt:
    currentModel: "qwen2.5:7b"  # Currently selected model
    availableModels: ["qwen2.5:7b", "llama3.2:3b", "deepseek-coder:6.7b"]  # User can switch between these
    defaultProvider: "ollama"

  # Provider configurations
  providers:
    # Local Ollama provider
    ollama:
      name: "Ollama"
      type: "local"
      enabled: true
      baseURL: "http://172.18.144.1:11434"
      # No API key required for local Ollama
      models:
        - name: "llama3.2:3b"
          displayName: "Llama 3.2 3B"
          maxTokens: 8192
          contextWindow: 128000
          capabilities: ["completion", "chat"]
          defaultParameters:
            temperature: 0.7
            top_p: 0.9
            top_k: 40
        - name: "qwen2.5:7b" 
          displayName: "Qwen 2.5 7B"
          maxTokens: 8192
          contextWindow: 32768
          capabilities: ["completion", "chat", "reasoning"]
          defaultParameters:
            temperature: 0.8
            top_p: 0.95
        - name: "deepseek-coder:6.7b"
          displayName: "DeepSeek Coder 6.7B"
          maxTokens: 4096
          contextWindow: 16384
          capabilities: ["completion", "chat", "code"]
          defaultParameters:
            temperature: 0.1
            top_p: 0.95
      healthCheck:
        enabled: true
        endpoint: "/api/tags"
        interval: 60000  # 1 minute
    
    # Groq - High-speed inference with free tier
    groq:
      name: "Groq"
      type: "api"
      enabled: true
      baseURL: "https://api.groq.com/openai/v1"
      apiKey: "${GROQ_API_KEY}"
      models:
        - name: "llama-3.1-8b-instant"
          displayName: "Llama 3.1 8B Instant"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 14400
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "llama-3.1-70b-versatile"
          displayName: "Llama 3.1 70B Versatile"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 6000
          defaultParameters:
            temperature: 0.8
            max_tokens: 1024
        - name: "mixtral-8x7b-32768"
          displayName: "Mixtral 8x7B"
          maxTokens: 32768
          contextWindow: 32768
          capabilities: ["completion", "chat", "multilingual"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 5000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 30
        tokensPerMinute: 14400

    # Hugging Face Inference API - Free tier available
    huggingface:
      name: "Hugging Face"
      type: "api"
      enabled: true
      baseURL: "https://api-inference.huggingface.co"
      apiKey: "${HUGGINGFACE_API_KEY}"
      models:
        - name: "microsoft/DialoGPT-medium"
          displayName: "DialoGPT Medium"
          maxTokens: 1024
          contextWindow: 1024
          capabilities: ["chat"]
          endpoint: "/models/microsoft/DialoGPT-medium"
          defaultParameters:
            max_length: 1000
            temperature: 0.7
        - name: "google/flan-t5-large"
          displayName: "Flan-T5 Large"
          maxTokens: 512
          contextWindow: 512
          capabilities: ["completion", "reasoning"]
          endpoint: "/models/google/flan-t5-large"
          defaultParameters:
            max_length: 200
            temperature: 0.7
        - name: "bigcode/starcoder"
          displayName: "StarCoder"
          maxTokens: 8192
          contextWindow: 8192
          capabilities: ["completion", "code"]
          endpoint: "/models/bigcode/starcoder"
          defaultParameters:
            max_length: 1000
            temperature: 0.1
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 1000  # Free tier limit
        tokensPerMinute: 10000

    # Together AI - Free tier with open source models
    together:
      name: "Together AI"
      type: "api"
      enabled: true
      baseURL: "https://api.together.xyz/v1"
      apiKey: "${TOGETHER_API_KEY}"
      models:
        - name: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
          displayName: "Llama 3.2 3B Instruct Turbo"
          maxTokens: 4096
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"
          displayName: "Llama 3.2 11B Vision Instruct"
          maxTokens: 4096
          contextWindow: 131072
          capabilities: ["completion", "chat", "vision"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "Qwen/Qwen2.5-7B-Instruct-Turbo"
          displayName: "Qwen 2.5 7B Instruct Turbo"
          maxTokens: 8192
          contextWindow: 32768
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.8
            max_tokens: 1024
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 600
        tokensPerMinute: 1000000

# Model selection strategies
strategies:
  # Cost optimization - prefer free/local models
  cost:
    priority: ["ollama", "huggingface", "groq", "together"]
    fallback: true
  
  # Speed optimization - prefer fastest inference
  speed:
    priority: ["groq", "together", "ollama", "huggingface"]
    fallback: true
  
  # Quality optimization - prefer most capable models
  quality:
    priority: ["together", "groq", "ollama", "huggingface"]
    fallback: true
  
  # Local-first - prefer local models, fallback to APIs
  local:
    priority: ["ollama"]
    fallback: ["groq", "together", "huggingface"]

# Monitoring and observability
monitoring:
  enabled: true
  metrics:
    enabled: true
    collectTokenUsage: true
    collectLatency: true
    collectErrorRates: true
  logging:
    level: "info"
    includeRequestBody: false
    includeSensitiveData: false
  healthChecks:
    enabled: true
    interval: 300000  # 5 minutes
    timeout: 5000

# Environment variable mappings
environment:
  variables:
    - name: "GROQ_API_KEY"
      required: false
      description: "API key for Groq inference service"
    - name: "HUGGINGFACE_API_KEY"
      required: false
      description: "API token for Hugging Face Inference API"
    - name: "TOGETHER_API_KEY"
      required: false
      description: "API key for Together AI service"
    - name: "OLLAMA_BASE_URL"
      required: false
      default: "http://172.18.144.1:11434"
      description: "Base URL for local Ollama instance"

# Security settings
security:
  apiKeyRotation:
    enabled: false
    interval: 2592000000  # 30 days
  requestValidation:
    enabled: true
    maxPromptLength: 10000
    sanitizeInput: true
  auditLogging:
    enabled: true
    logRequests: true
    logResponses: false