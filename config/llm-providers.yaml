# Multi-Provider LLM Configuration
# This file configures multiple LLM providers including local (Ollama) and free API providers

llm:
  # Default provider selection strategy
  defaultProvider: "ollama"
  
  # Fallback chain - providers to try in order if primary fails
  fallbackChain: ["ollama", "openrouter", "groq", "huggingface", "together"]
  
  # Request timeout and retry configuration
  timeout: 30000  # 30 seconds
  maxRetries: 3
  retryDelay: 1000  # 1 second
  
  # Rate limiting configuration
  rateLimit:
    enabled: true
    requestsPerMinute: 60
    tokensPerMinute: 100000

  # LLM configuration for input classification
  classifier:
    provider: "ollama"
    model: "llama3.2:3b"  # Use available model for classification
    temperature: 0.1     # Low temperature for consistent classification
    maxTokens: 1000      # Increased for structured output with reasoning

  # LLM configuration for user prompts (user-changeable)
  prompt:
    currentModel: "llama3.2:3b"  # Start with local model
    defaultProvider: "ollama"

  # Provider configurations
  providers:
    # Local Ollama provider
    ollama:
      name: "Ollama"
      type: "local"
      enabled: true
      baseURL: "http://localhost:11434"
      # No API key required for local Ollama
      models:
        - name: "llama3.2:3b"
          displayName: "Llama 3.2 3B"
          maxTokens: 8192
          contextWindow: 128000
          capabilities: ["completion", "chat"]
          defaultParameters:
            temperature: 0.7
            top_p: 0.9
            top_k: 40
        - name: "qwen3:0.6b" 
          displayName: "Qwen 3 0.6B"
          maxTokens: 8192
          contextWindow: 32768
          capabilities: ["completion", "chat", "reasoning"]
          defaultParameters:
            temperature: 0.8
            top_p: 0.95
        - name: "qwen3-coder:latest"
          displayName: "Qwen 3 Coder Latest"
          maxTokens: 8192
          contextWindow: 256000
          capabilities: ["completion", "chat", "coding", "reasoning", "function_calling"]
          defaultParameters:
            temperature: 0.1
            top_p: 0.95
            top_k: 40
        - name: "qwen3:32b"
          displayName: "Qwen 3 32B"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "reasoning", "thinking"]
          defaultParameters:
            temperature: 0.7
            top_p: 0.9
            top_k: 40
      healthCheck:
        enabled: true
        endpoint: "/api/tags"
        interval: 60000  # 1 minute
    
    # OpenRouter - Access to multiple AI models through single API
    openrouter:
      name: "OpenRouter"
      type: "api"
      enabled: true
      baseURL: "https://openrouter.ai/api/v1"
      apiKey: "${OPENROUTER_API_KEY}"
      models:
        - name: "anthropic/claude-3.5-haiku"
          displayName: "Claude 3.5 Haiku"
          maxTokens: 8192
          contextWindow: 200000
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 500
            tokensPerMinute: 100000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "meta-llama/llama-3.2-3b-instruct"
          displayName: "Llama 3.2 3B Instruct"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 200
            tokensPerMinute: 40000
          defaultParameters:
            temperature: 0.8
            max_tokens: 2048
        - name: "google/gemini-flash-1.5"
          displayName: "Gemini Flash 1.5"
          maxTokens: 8192
          contextWindow: 1048576
          capabilities: ["completion", "chat", "vision", "multimodal"]
          rateLimit:
            requestsPerMinute: 1000
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.9
            max_tokens: 4096
        - name: "openai/gpt-4o-mini"
          displayName: "GPT-4o Mini"
          maxTokens: 16384
          contextWindow: 128000
          capabilities: ["completion", "chat", "reasoning", "vision"]
          rateLimit:
            requestsPerMinute: 500
            tokensPerMinute: 200000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "meta-llama/llama-3.2-3b-instruct:free"
          displayName: "Llama 3.2 3B Instruct (Free)"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 200
            tokensPerMinute: 40000
          defaultParameters:
            temperature: 0.8
            max_tokens: 2048
        - name: "meta-llama/llama-3.2-1b-instruct:free"
          displayName: "Llama 3.2 1B Instruct (Free)"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 200
            tokensPerMinute: 40000
          defaultParameters:
            temperature: 0.8
            max_tokens: 2048
        - name: "microsoft/phi-3-mini-128k-instruct:free"
          displayName: "Phi-3 Mini 128K (Free)"
          maxTokens: 8192
          contextWindow: 128000
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 1000
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "qwen/qwen3-coder"
          displayName: "Qwen3 Coder"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "coding"]
          rateLimit:
            requestsPerMinute: 100
            tokensPerMinute: 50000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "qwen/qwen3-coder:free"
          displayName: "Qwen3 Coder (Free)"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "coding"]
          rateLimit:
            requestsPerMinute: 200
            tokensPerMinute: 100000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "qwen/qwen3-32b-coder"
          displayName: "Qwen3 32B Coder"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "coding", "reasoning"]
          rateLimit:
            requestsPerMinute: 50
            tokensPerMinute: 25000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "qwen/qwen3-32b"
          displayName: "Qwen3 32B"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "reasoning", "thinking"]
          rateLimit:
            requestsPerMinute: 60
            tokensPerMinute: 30000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
        - name: "qwen/qwen3-32b:free"
          displayName: "Qwen3 32B (Free)"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "reasoning", "thinking"]
          rateLimit:
            requestsPerMinute: 100
            tokensPerMinute: 50000
          defaultParameters:
            temperature: 0.7
            max_tokens: 4096
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 500
        tokensPerMinute: 100000

    # Groq - High-speed inference with free tier
    groq:
      name: "Groq"
      type: "api"
      enabled: true
      baseURL: "https://api.groq.com/openai/v1"
      apiKey: "${GROQ_API_KEY}"
      models:
        - name: "llama-3.1-8b-instant"
          displayName: "Llama 3.1 8B Instant"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 14400
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "llama-3.1-70b-versatile"
          displayName: "Llama 3.1 70B Versatile"
          maxTokens: 8192
          contextWindow: 131072
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 6000
          defaultParameters:
            temperature: 0.8
            max_tokens: 1024
        - name: "mixtral-8x7b-32768"
          displayName: "Mixtral 8x7B"
          maxTokens: 32768
          contextWindow: 32768
          capabilities: ["completion", "chat", "multilingual"]
          rateLimit:
            requestsPerMinute: 30
            tokensPerMinute: 5000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 30
        tokensPerMinute: 14400

    # Hugging Face Inference API - Free tier available
    huggingface:
      name: "Hugging Face"
      type: "api"
      enabled: true
      baseURL: "https://api-inference.huggingface.co"
      apiKey: "${HUGGINGFACE_API_KEY}"
      models:
        - name: "microsoft/DialoGPT-medium"
          displayName: "DialoGPT Medium"
          maxTokens: 1024
          contextWindow: 1024
          capabilities: ["chat"]
          endpoint: "/models/microsoft/DialoGPT-medium"
          defaultParameters:
            max_length: 1000
            temperature: 0.7
        - name: "google/flan-t5-large"
          displayName: "Flan-T5 Large"
          maxTokens: 512
          contextWindow: 512
          capabilities: ["completion", "reasoning"]
          endpoint: "/models/google/flan-t5-large"
          defaultParameters:
            max_length: 200
            temperature: 0.7
        - name: "bigcode/starcoder"
          displayName: "StarCoder"
          maxTokens: 8192
          contextWindow: 8192
          capabilities: ["completion", "code"]
          endpoint: "/models/bigcode/starcoder"
          defaultParameters:
            max_length: 1000
            temperature: 0.1
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 1000  # Free tier limit
        tokensPerMinute: 10000

    # Together AI - Free tier with open source models
    together:
      name: "Together AI"
      type: "api"
      enabled: true
      baseURL: "https://api.together.xyz/v1"
      apiKey: "${TOGETHER_API_KEY}"
      models:
        - name: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
          displayName: "Llama 3.2 3B Instruct Turbo"
          maxTokens: 4096
          contextWindow: 131072
          capabilities: ["completion", "chat"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"
          displayName: "Llama 3.2 11B Vision Instruct"
          maxTokens: 4096
          contextWindow: 131072
          capabilities: ["completion", "chat", "vision"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.7
            max_tokens: 1024
        - name: "Qwen/Qwen2.5-7B-Instruct-Turbo"
          displayName: "Qwen 2.5 7B Instruct Turbo"
          maxTokens: 8192
          contextWindow: 32768
          capabilities: ["completion", "chat", "reasoning"]
          rateLimit:
            requestsPerMinute: 600
            tokensPerMinute: 1000000
          defaultParameters:
            temperature: 0.8
            max_tokens: 1024
      authentication:
        type: "bearer"
        headerName: "Authorization"
      rateLimit:
        requestsPerMinute: 600
        tokensPerMinute: 1000000

# Model selection strategies
strategies:
  # Cost optimization - prefer free/local models
  cost:
    priority: ["ollama", "huggingface", "openrouter", "groq", "together"]
    fallback: true
  
  # Speed optimization - prefer fastest inference
  speed:
    priority: ["groq", "openrouter", "together", "ollama", "huggingface"]
    fallback: true
  
  # Quality optimization - prefer most capable models
  quality:
    priority: ["openrouter", "together", "groq", "ollama", "huggingface"]
    fallback: true
  
  # Local-first - prefer local models, fallback to APIs
  local:
    priority: ["ollama"]
    fallback: ["openrouter", "groq", "together", "huggingface"]

# Monitoring and observability
monitoring:
  enabled: true
  metrics:
    enabled: true
    collectTokenUsage: true
    collectLatency: true
    collectErrorRates: true
  logging:
    level: "info"
    includeRequestBody: false
    includeSensitiveData: false
  healthChecks:
    enabled: true
    interval: 300000  # 5 minutes
    timeout: 5000

# Environment variable mappings
environment:
  variables:
    - name: "OPENROUTER_API_KEY"
      required: false
      description: "API key for OpenRouter service (access to Claude, GPT, Gemini, etc.)"
    - name: "GROQ_API_KEY"
      required: false
      description: "API key for Groq inference service"
    - name: "HUGGINGFACE_API_KEY"
      required: false
      description: "API token for Hugging Face Inference API"
    - name: "TOGETHER_API_KEY"
      required: false
      description: "API key for Together AI service"
    - name: "OLLAMA_BASE_URL"
      required: false
      default: "http://172.18.144.1:11434"
      description: "Base URL for local Ollama instance"

# Security settings
security:
  apiKeyRotation:
    enabled: false
    interval: 2592000000  # 30 days
  requestValidation:
    enabled: true
    maxPromptLength: 100000
    sanitizeInput: true
  auditLogging:
    enabled: true
    logRequests: true
    logResponses: false